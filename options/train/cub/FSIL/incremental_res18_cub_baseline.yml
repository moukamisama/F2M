#################
# General Setting
#################
# Experiment name, if the experiment name has the word debug, it will enter debug mode
name: MTNBModel_res18_cub_5shots_100base_11tasks_NormalPretrain{1e-3_1e-2_600_900_1200}_debug
# The type of model used, usually the class name of the model defined in the `methods` directory
model_type: BaselineModel
# gpu
gpu: '1'
# Random seed
manual_seed: 1997
use_cosine: false
Random: false

#################################
# The settings for the dataset and data loader
#################################
transformer_agu:
  - type: RandomResizedCrop
    size: 224
  - type: RandomHorizontalFlip
  - type: ToTensor
  - type: Normalize
    mean: !!python/tuple [0.485, 0.456, 0.406]
    std: !!python/tuple [0.229, 0.224, 0.225]

transformer:
  - type: Resize
    size: 256
  - type: CenterCrop
    size: 224
  - type: ToTensor
  - type: Normalize
    mean: !!python/tuple [ 0.485, 0.456, 0.406 ]
    std: !!python/tuple [ 0.229, 0.224, 0.225 ]

datasets:
  # The important information of dataset
  train:
    name: cub
    type: NormalDataset
    total_classes: 200
    dataroot: DataSet/CUB_200_2011/train
    aug: false
    # Number of processes read by the data loader per GPU
    num_worker_per_gpu: 8
    # Batch size
    batch_size: ~
    pin_memory: true
    pre_load: false

    sampler:
      type: ~

  val:
    name: cub
    type: NormalDataset
    total_classes: 200
    dataroot: DataSet/CUB_200_2011/test
    aug: false
    # Number of processes read by the data loader per GPU
    num_worker_per_gpu: 8
    pin_memory: true
    pre_load: false

  test:
    name: cub
    type: NormalDataset
    total_classes: 200
    dataroot: DataSet/CUB_200_2011/test
    aug: false
    # Number of processes read by the data loader per GPU
    num_worker_per_gpu: 8
    pin_memory: true
    pre_load: false

#####################
# The following is the setup of the network structure
#####################
network_g:
  type: Resnet18_softmax
  Embed_dim: 512
  pretrained: false
  norm: false
  num_classes: 100
  adopt_classifier: false
  addition_net: false
  flatten: true
  cut_at_pooling: false

######################################
# The following are the paths
######################################
path:
  # Base model path, need models ending in pth
  pretrain_model_g: ./exp/cub_bases100/FSIL_CFRPModel_res18_CUB_baseline_100base_NormalPretrain_{1e3_1e2_80_160_220}_wandb_001/models/best_net_latest.pth
  base_model: ./exp/cub_bases100/FSIL_CFRPModel_res18_CUB_baseline_100base_NormalPretrain_{1e3_1e2_80_160_220}_wandb_001/models/best_net_latest.pth
  strict_load: false


#################
# The following is the training setup
#################
train:
  # Optimizer
  optim_g:
    # The type of optimizer
    type: SGD
    ##### The following properties are flexible and have different settings depending on the optimizer
    # learning rate
    lr: !!float 2e-2
    weight_decay: !!float 5e-4
    # momentum
    momentum: !!float 0.9

  # The setting of scheduler
  scheduler:
    # The type of scheduler
    type: MultiStepLR
    #### The following properties are flexible, depending on the learning rate Scheduler has different settings
    #
    # milestones, using epoch to represent the points to decay the learning rate
    milestones: [8]

    # gama
    gamma: !!float 1e-6

  novel_exemplars: 5
  # whether fine_tune
  fine_tune: false
  feat_buffer: false
  calculate_pt_with_buffer: false

  # The number of warm up iteration, if -1, means no warm up
  warmup_iter: -1  # no warm up
  # number of base classes
  bases: 100
  # number of tasks for incremental few-shot learning
  tasks: 11
  # number of shots for incremental few-shot learning
  shots: 5
  # number of tests for incremental few-shot learning
  num_test: 5

######################
# 以下为Validation的设置
#######################
val:
  # The frequency of validation
  val_freq: 1
  test_type: NCM
  use_all_bases: false

####################
# The following are the settings for Logging
####################
logger:
  # Frequency of loggers printed on the screen according to iteration
  print_freq: 1
  # Whether to use tensorboard logger

  use_tb_logger: true
  # Whether to use wandb logger
  wandb:
    # The default is None, i.e. wandb is not used.
    project: FIL-Noise-Cub-Incremental
    # If it is resume, you can enter the last wandb id, then the log can be connected
    resume_id: ~

